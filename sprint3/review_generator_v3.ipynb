{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning GPT-2 to generate reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm, trange\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "# from langdetect import detect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in all the data and preparing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(\"../tripadvisor_dataset/reviews.csv\")\n",
    "reviews = reviews.applymap(str) # convert to string because there are some rows with float"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the English reviews because gpt is an English model\n",
    "\n",
    "Code can be found here: https://www.kaggle.com/hikmatelhaj/extracting-english-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0    12647\n",
       "4.0     8595\n",
       "3.0     2744\n",
       "1.0     1486\n",
       "2.0     1351\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = pd.read_csv(\"reviews_en.csv\")\n",
    "reviews[\"rating\"].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take 1000 5-star and 1000 1-star reviews to finetune the model. We don't use more data because it takes a while to finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.concat([reviews.query(\"rating == 1.0\").sample(1000),reviews.query(\"rating == 5.0\").sample(1000)])\n",
    "reviews = reviews.reset_index() # reset the indices after sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>reviewer name</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25933</td>\n",
       "      <td>4431449</td>\n",
       "      <td>Madelon_B70</td>\n",
       "      <td>Terrible disappointment</td>\n",
       "      <td>May 24, 2016</td>\n",
       "      <td>Based on a recommendation about the location a...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22227</td>\n",
       "      <td>10643135</td>\n",
       "      <td>224mariias</td>\n",
       "      <td>Order your coffee someplace else</td>\n",
       "      <td>January 11, 2020</td>\n",
       "      <td>Yesterday came here and ordered three coffees....</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26660</td>\n",
       "      <td>784382</td>\n",
       "      <td>Babslalala</td>\n",
       "      <td>Terrible!</td>\n",
       "      <td>December 8, 2018</td>\n",
       "      <td>I usualy don’t give negative reviews but this ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22993</td>\n",
       "      <td>6878696</td>\n",
       "      <td>boblecostaud</td>\n",
       "      <td>Avoid</td>\n",
       "      <td>August 29, 2019</td>\n",
       "      <td>Customer service does not exist here. We polit...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9819</td>\n",
       "      <td>3676410</td>\n",
       "      <td>Gregor B</td>\n",
       "      <td>Terrible food and service</td>\n",
       "      <td>April 3, 2016</td>\n",
       "      <td>We came here to have dinner and watch a footba...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>8117</td>\n",
       "      <td>1058490</td>\n",
       "      <td>paratanytarsus</td>\n",
       "      <td>Fabulous Fondue</td>\n",
       "      <td>May 6, 2014</td>\n",
       "      <td>We returned to this restaurant with son Paul a...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1665</td>\n",
       "      <td>967590</td>\n",
       "      <td>A Tripadvisor reviewer on Facebook</td>\n",
       "      <td>verry nice food</td>\n",
       "      <td>June 11, 2008</td>\n",
       "      <td>verry nice food</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1301</td>\n",
       "      <td>740670</td>\n",
       "      <td>ppaulmm</td>\n",
       "      <td>A jewel of a restaurant in a jewel of a city</td>\n",
       "      <td>September 12, 2016</td>\n",
       "      <td>We visited Ghent for a few days – a great city...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>8345</td>\n",
       "      <td>1894818</td>\n",
       "      <td>Caroline026</td>\n",
       "      <td>Cosy place with excellent food</td>\n",
       "      <td>July 28, 2016</td>\n",
       "      <td>As soon as you are seated in this cosy restaur...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>6175</td>\n",
       "      <td>15194988</td>\n",
       "      <td>AngeloP2608</td>\n",
       "      <td>Check this out!</td>\n",
       "      <td>October 15, 2018</td>\n",
       "      <td>As babybrother from Oak, this restaurant is re...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index        id                       reviewer name  \\\n",
       "0     25933   4431449                         Madelon_B70   \n",
       "1     22227  10643135                          224mariias   \n",
       "2     26660    784382                          Babslalala   \n",
       "3     22993   6878696                        boblecostaud   \n",
       "4      9819   3676410                            Gregor B   \n",
       "...     ...       ...                                 ...   \n",
       "1995   8117   1058490                      paratanytarsus   \n",
       "1996   1665    967590  A Tripadvisor reviewer on Facebook   \n",
       "1997   1301    740670                             ppaulmm   \n",
       "1998   8345   1894818                         Caroline026   \n",
       "1999   6175  15194988                         AngeloP2608   \n",
       "\n",
       "                                             title                date  \\\n",
       "0                          Terrible disappointment        May 24, 2016   \n",
       "1                Order your coffee someplace else     January 11, 2020   \n",
       "2                                        Terrible!    December 8, 2018   \n",
       "3                                            Avoid     August 29, 2019   \n",
       "4                        Terrible food and service       April 3, 2016   \n",
       "...                                            ...                 ...   \n",
       "1995                               Fabulous Fondue         May 6, 2014   \n",
       "1996                               verry nice food       June 11, 2008   \n",
       "1997  A jewel of a restaurant in a jewel of a city  September 12, 2016   \n",
       "1998                Cosy place with excellent food       July 28, 2016   \n",
       "1999                              Check this out!     October 15, 2018   \n",
       "\n",
       "                                                 review  rating  \n",
       "0     Based on a recommendation about the location a...     1.0  \n",
       "1     Yesterday came here and ordered three coffees....     1.0  \n",
       "2     I usualy don’t give negative reviews but this ...     1.0  \n",
       "3     Customer service does not exist here. We polit...     1.0  \n",
       "4     We came here to have dinner and watch a footba...     1.0  \n",
       "...                                                 ...     ...  \n",
       "1995  We returned to this restaurant with son Paul a...     5.0  \n",
       "1996                                    verry nice food     5.0  \n",
       "1997  We visited Ghent for a few days – a great city...     5.0  \n",
       "1998  As soon as you are seated in this cosy restaur...     5.0  \n",
       "1999  As babybrother from Oak, this restaurant is re...     5.0  \n",
       "\n",
       "[2000 rows x 7 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a dataset which tokenizes the reviews. We also limit the review length to 1024 tokens in case a review is longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewData(Dataset):\n",
    "    def __init__(self, control_code, gpt2_type=\"gpt2\", max_length=1024):\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n",
    "        self.revs = []\n",
    "        for row in reviews['review']:\n",
    "            self.revs.append(torch.tensor(\n",
    "                self.tokenizer.encode(f\"<|{control_code}|>{row[:max_length]}<|endoftext|>\")\n",
    "            ))\n",
    "        self.rev_count = len(self.revs)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.rev_count\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.revs[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58001f7ec992481da40fb282ee3d6f91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adbe67a15abf498f8604e650aa2728a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a48ff3eab18a43c9a05934d3615853d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = ReviewData(reviews['review'], gpt2_type=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b6f852c7d6348b7845a60f3422eec94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT is a huge model, to limit the calculation. Before performing a gradient descent step, it'll sum up all the gradients of several operations. Then it will divide that sum by the number of accumulated steps, to get an average loss over the training sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accumulated batch size\n",
    "\n",
    "def pack_tensor(new_tensor, packed_tensor, max_seq_len):\n",
    "    if packed_tensor is None:\n",
    "        return new_tensor, True, None\n",
    "    if new_tensor.size()[1] + packed_tensor.size()[1] > max_seq_len:\n",
    "        return packed_tensor, False, new_tensor\n",
    "    else:\n",
    "        packed_tensor = torch.cat([new_tensor, packed_tensor[:, 1:]], dim=1)\n",
    "        return packed_tensor, True, None\n",
    "     "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "The hyperparameters to tune are learning rate, batch size, epochs and optimizer (ADAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def train(\n",
    "    dataset, model, tokenizer,\n",
    "    batch_size=16, epochs=10, lr=2e-5,\n",
    "    max_seq_len=400, warmup_steps=200,\n",
    "    gpt2_type=\"gpt2\", output_dir=\".\", output_prefix=\"wreckgar\",\n",
    "    test_mode=False, save_model_on_epoch=False,\n",
    "):\n",
    "\n",
    "    acc_steps = 100\n",
    "    device=torch.device(\"cuda\")\n",
    "    model = model.cuda()\n",
    "    model.train()\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "    loss=0\n",
    "    accumulating_batch_count = 0\n",
    "    input_tensor = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        print(f\"Training epoch {epoch}\")\n",
    "        print(loss)\n",
    "        for idx, entry in tqdm(enumerate(train_dataloader)):\n",
    "            (input_tensor, carry_on, remainder) = pack_tensor(entry, input_tensor, 768)\n",
    "\n",
    "            if carry_on and idx != len(train_dataloader) - 1:\n",
    "                continue\n",
    "\n",
    "            input_tensor = input_tensor.to(device)\n",
    "            outputs = model(input_tensor, labels=input_tensor)\n",
    "            loss = outputs[0]\n",
    "            loss.backward()\n",
    "\n",
    "            if (accumulating_batch_count % batch_size) == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                model.zero_grad()\n",
    "\n",
    "            accumulating_batch_count += 1\n",
    "            input_tensor = None\n",
    "        if save_model_on_epoch:\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                os.path.join(output_dir, f\"{output_prefix}-{epoch}.pt\"),\n",
    "            )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [01:20, 24.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1\n",
      "tensor(2.7957, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [01:20, 24.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 2\n",
      "tensor(1.1757, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [01:22, 24.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 3\n",
      "tensor(1.0292, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [01:20, 24.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 4\n",
      "tensor(1.3510, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [01:21, 24.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 5\n",
      "tensor(0.8160, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [01:24, 23.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 6\n",
      "tensor(1.4777, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [01:21, 24.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 7\n",
      "tensor(0.7970, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [01:20, 24.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 8\n",
      "tensor(0.6082, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [01:22, 24.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 9\n",
      "tensor(0.9392, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [01:21, 24.49it/s]\n"
     ]
    }
   ],
   "source": [
    "model = train(dataset, model, tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating\n",
    "\n",
    "Now generating the text has some parameters.\n",
    "\n",
    "entry_count: The amount of times to generate\n",
    "\n",
    "entry_length: Maximum amount of words to generate\n",
    "\n",
    "top_p: The minimum probablity to filter possible outcomes. An example:\n",
    "```\n",
    "Probability as sequence of this sentence: it's hot ...\n",
    "1% today\n",
    "2% yesterday\n",
    "3% tomorrow\n",
    "...\n",
    "\n",
    "This will be converted to\n",
    "1% today\n",
    "3% today, yeserday\n",
    "6% today, yesterday, tomorrow\n",
    "...\n",
    "\n",
    "If we now set the top_p to 1%, then the chance that the word 'today' get's chosen is higher than when we put top_p to 6%. A low top_p will then result in more 'randomness', more chance that a less likely word gets chosen.\n",
    "\n",
    "```\n",
    "\n",
    "temperature: The same parameter that we used in our previous model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:08<00:00,  8.56s/it]\n"
     ]
    }
   ],
   "source": [
    "def generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    entry_count=10,\n",
    "    entry_length=30,\n",
    "    top_p=0.8,\n",
    "    temperature=0.5,\n",
    "):\n",
    "    model.eval()\n",
    "    generated_num = 0\n",
    "    generated_list = []\n",
    "    # print(f\"temperature is {temperature}\")\n",
    "    filter_value = -float(\"Inf\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for entry_idx in trange(entry_count):\n",
    "\n",
    "            entry_finished = False\n",
    "            generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "\n",
    "            for i in range(entry_length):\n",
    "                outputs = model(generated, labels=generated)\n",
    "                loss, logits = outputs[:2]\n",
    "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
    "                    ..., :-1\n",
    "                ].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                logits[:, indices_to_remove] = filter_value\n",
    "\n",
    "                next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
    "                generated = torch.cat((generated, next_token), dim=1)\n",
    "\n",
    "                if next_token in tokenizer.encode(\"<|endoftext|>\"):\n",
    "                    entry_finished = True\n",
    "\n",
    "                if entry_finished:\n",
    "\n",
    "                    generated_num = generated_num + 1\n",
    "\n",
    "                    output_list = list(generated.squeeze().numpy())\n",
    "                    output_text = tokenizer.decode(output_list)\n",
    "                    generated_list.append(output_text)\n",
    "                    break\n",
    "            \n",
    "            if not entry_finished:\n",
    "              output_list = list(generated.squeeze().numpy())\n",
    "              output_text = f\"{tokenizer.decode(output_list)}<|endoftext|>\" \n",
    "              generated_list.append(output_text)\n",
    "                \n",
    "    return generated_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_generation_tekst_meegeven(text, temperature=0.5):\n",
    "    x = generate(model.to('cpu'), tokenizer, text, entry_count=1, temperature=temperature)\n",
    "    return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first try to generate positive reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the food was delicious and the service was great. The only thing I would change is the menu. I would definitely recommend this place again.<|endoftext|>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['the food was delicious and the service was great. The only thing I would change is the menu. I would definitely recommend this place again.<|endoftext|>']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_generation_tekst_meegeven(\"the food was delicious\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"We had a great time and we'll be back next year.<|endoftext|>\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[\"We had a great time and we'll be back next year.<|endoftext|>\"]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_generation_tekst_meegeven(\"We had a great time\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating positive reviews seems to work great, now let's test the negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperature is 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  7.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['too expensive food and meals were cooked in the house.\\n\\nDespite this, the family was able to get some lovely restaurants that would sell you their meals in their place.<|endoftext|>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['too expensive food and meals were cooked in the house.\\n\\nDespite this, the family was able to get some lovely restaurants that would sell you their meals in their place.<|endoftext|>']]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_generation_tekst_meegeven(\"too expensive food and meals\", 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperature is 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:09<00:00,  9.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bad food       I went with a group of friends and the service...\\n1 person found this review helpful.\\n\\nReviewed By Date Rating<|endoftext|>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['bad food       I went with a group of friends and the service...\\n1 person found this review helpful.\\n\\nReviewed By Date Rating<|endoftext|>']]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_generation_tekst_meegeven(\"bad food \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperature is 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:08<00:00,  8.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dry food and too salty \\xa0for me.\\nI was very disappointed with the quality of the food. The only thing I could think of was that it was not a good<|endoftext|>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['dry food and too salty \\xa0for me.\\nI was very disappointed with the quality of the food. The only thing I could think of was that it was not a good<|endoftext|>']]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_generation_tekst_meegeven(\"dry food and too salty \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  7.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['impolite staff, who were all in the building.\\n\\n\"I was really upset and scared, I was really upset,\" said one woman. \"I was<|endoftext|>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['impolite staff, who were all in the building.\\n\\n\"I was really upset and scared, I was really upset,\" said one woman. \"I was<|endoftext|>']]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_generation_tekst_meegeven(\"impolite staff\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The negative reviews are much harder to generate, even though we have as much positive as negative reviews. To explain this we take a deeper look at the negative reviews. We save it to a csv and we scroll through the negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.where(reviews.rating == 1.0).dropna()[\"review\"].to_csv(\"bad.csv\", index=False)\n",
    "reviews.where(reviews.rating == 5.0).dropna()[\"review\"].to_csv(\"positive.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Had the ribs....terrible. \\nImagine chewing on...\n",
       "1    Ate there twice in ten years. The first time m...\n",
       "2    Okay. Let me start by saying that I went to th...\n",
       "3    My wife and I ordered 2 burgers, they took 30 ...\n",
       "4     Kids wanted Italian so we stopped at this res...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.where(reviews.rating == 1.0)[\"review\"].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000    Especially the friendliness and happiness from...\n",
       "1001    One of the best in GentIf you want to taste an...\n",
       "1002    We found restaurant food in Ghent to be very e...\n",
       "1003    Best price/quality veggie in Ghent.Appelier is...\n",
       "1004    Fab ribs & baked potatoes. The waiters are alw...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.where(reviews.rating == 5.0).dropna()[\"review\"].head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After observing the positive and negative reviews, we saw that the negative reviews are much wider and specific than the positive reviews. For example the positive reviews will mostly say that the food is great, that the staff is great etc. \n",
    "\n",
    "Negative reviews are complexer, people complain more about specific things and not an overall review.\n",
    "\n",
    "We have also tried generating reviews such as \"not good\" or \"not tasteful\", but still generates a positive review. We think that the 'not' doesn't have the expected result of making the word the opposite. It works better when you use words such as \"bad\" or \"tasteless\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This model is a good improvement in comparison with our selfmade model, but it isn't working perfect.\n",
    "\n",
    "We think that the problem is that we don't have enough negative reviews to be able to generate quality negative reviews. It takes more data to learn negative reviews because negative reviews are harder to generalize than positive reviews. We think that adding more negative reviews and training more epochs will make the model much better."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "https://towardsdatascience.com/how-to-fine-tune-gpt-2-for-text-generation-ae2ea53bc272"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "218548c374b4bbf341f954c1c86cc69d1fe99eef78085dfb9916d33ba2c70687"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
