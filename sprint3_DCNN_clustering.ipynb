{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCNN clustering techniques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In sprint2 we tried a lot of different feature extraction techniques like SIFT, HOG, SURF to improve the clustering of pictures of different dishes. However the result was not as hoped and the cluster were very poor.\n",
    "\n",
    "We will try to improve this with useing Deep Convolutionals Neural Networks to try to dynamically learn the specific features and improve the clustering performance.\n",
    "\n",
    "source: https://towardsdatascience.com/image-clustering-implementation-with-pytorch-587af1d14123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## auto-encoding\n",
    "TODO: steps of auto encoding in eigen woorden uitleggen en auto encoding uitleggen\n",
    "\n",
    "We will create the Encoder\n",
    "\n",
    "Uitleggen waarom 2 lagen verwijderd zijn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "\n",
    "class EncoderVGG(nn.Module):\n",
    "    '''Encoder of image based on the architecture of VGG-16 with batch normalization.\n",
    "    Args:\n",
    "        pretrained_params (bool, optional): If the network should be populated with pre-trained VGG parameters.\n",
    "            Defaults to True.\n",
    "    '''\n",
    "    channels_in = 3\n",
    "    channels_code = 512\n",
    "\n",
    "    def __init__(self, pretrained_params=True):\n",
    "        super(EncoderVGG, self).__init__()\n",
    "\n",
    "        vgg = models.vgg16_bn(pretrained=pretrained_params)\n",
    "        del vgg.classifier\n",
    "        del vgg.avgpool\n",
    "\n",
    "        self.encoder = self._encodify_(vgg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pooling indices\n",
    "uitleg _encodify zetten en indices van max-pooling krijgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderVGG(EncoderVGG):\n",
    "    def _encodify_(self, encoder):\n",
    "        '''Create list of modules for encoder based on the architecture in VGG template model.\n",
    "        In the encoder-decoder architecture, the unpooling operations in the decoder require pooling\n",
    "        indices from the corresponding pooling operation in the encoder. In VGG template, these indices\n",
    "        are not returned. Hence the need for this method to extent the pooling operations.\n",
    "        Args:\n",
    "            encoder : the template VGG model\n",
    "        Returns:\n",
    "            modules : the list of modules that define the encoder corresponding to the VGG model\n",
    "        '''\n",
    "        modules = nn.ModuleList()\n",
    "        for module in encoder.features:\n",
    "            if isinstance(module, nn.MaxPool2d):\n",
    "                module_add = nn.MaxPool2d(kernel_size=module.kernel_size,\n",
    "                                            stride=module.stride,\n",
    "                                            padding=module.padding,\n",
    "                                            return_indices=True)\n",
    "                modules.append(module_add)\n",
    "            else:\n",
    "                modules.append(module)\n",
    "\n",
    "        return modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward mini-batch through encoder of _encodify\n",
    "uitleg hoe pool indices in forward worden opgeslagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderVGG(EncoderVGG):\n",
    "    def forward(self, x):\n",
    "        '''Execute the encoder on the image input\n",
    "        Args:\n",
    "            x (Tensor): image tensor\n",
    "        Returns:\n",
    "            x_code (Tensor): code tensor\n",
    "            pool_indices (list): Pool indices tensors in order of the pooling modules\n",
    "        '''\n",
    "        pool_indices = []\n",
    "        x_current = x\n",
    "        for module_encode in self.encoder:\n",
    "            output = module_encode(x_current)\n",
    "\n",
    "            # If the module is pooling, there are two outputs, the second the pool indices\n",
    "            if isinstance(output, tuple) and len(output) == 2:\n",
    "                x_current = output[0]\n",
    "                pool_indices.append(output[1])\n",
    "            else:\n",
    "                x_current = output\n",
    "\n",
    "        return x_current, pool_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "uitleggen dat dit eigenlijk zelfde structuur als encoder is maar dan getransponeerd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderVGG(nn.Module):\n",
    "    '''Decoder of code based on the architecture of VGG-16 with batch normalization.\n",
    "    Args:\n",
    "        encoder: The encoder instance of `EncoderVGG` that is to be inverted into a decoder\n",
    "    '''\n",
    "    channels_in = EncoderVGG.channels_code\n",
    "    channels_out = 3\n",
    "\n",
    "    def __init__(self, encoder):\n",
    "        super(DecoderVGG, self).__init__()\n",
    "\n",
    "        self.decoder = self._invert_(encoder)\n",
    "        \n",
    "    def _invert_(self, encoder):\n",
    "        '''Invert the encoder in order to create the decoder as a (more or less) mirror image of the encoder\n",
    "        The decoder is comprised of two principal types: the 2D transpose convolution and the 2D unpooling. The 2D transpose\n",
    "        convolution is followed by batch normalization and activation. Therefore as the module list of the encoder\n",
    "        is iterated over in reverse, a convolution in encoder is turned into transposed convolution plus normalization\n",
    "        and activation, and a maxpooling in encoder is turned into unpooling.\n",
    "        Args:\n",
    "            encoder (ModuleList): the encoder\n",
    "        Returns:\n",
    "            decoder (ModuleList): the decoder obtained by \"inversion\" of encoder\n",
    "        '''\n",
    "        modules_transpose = []\n",
    "        for module in reversed(encoder):\n",
    "\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                kwargs = {'in_channels' : module.out_channels, 'out_channels' : module.in_channels,\n",
    "                          'kernel_size' : module.kernel_size, 'stride' : module.stride,\n",
    "                          'padding' : module.padding}\n",
    "                module_transpose = nn.ConvTranspose2d(**kwargs)\n",
    "                module_norm = nn.BatchNorm2d(module.in_channels)\n",
    "                module_act = nn.ReLU(inplace=True)\n",
    "                modules_transpose += [module_transpose, module_norm, module_act]\n",
    "\n",
    "            elif isinstance(module, nn.MaxPool2d):\n",
    "                kwargs = {'kernel_size' : module.kernel_size, 'stride' : module.stride,\n",
    "                          'padding' : module.padding}\n",
    "                module_transpose = nn.MaxUnpool2d(**kwargs)\n",
    "                modules_transpose += [module_transpose]\n",
    "\n",
    "        # Discard the final normalization and activation, so final module is convolution with bias\n",
    "        modules_transpose = modules_transpose[:-2]\n",
    "\n",
    "        return nn.ModuleList(modules_transpose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## forward through decoder\n",
    "\n",
    "uitleggen adhv eerste afbeelding dat alles doorlopen is \n",
    "Ook \"Code\" van figuur extra uitleggen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderVGG(DecoderVGG):  \n",
    "    def forward(self, x, pool_indices):\n",
    "        '''Execute the decoder on the code tensor input\n",
    "        Args:\n",
    "            x (Tensor): code tensor obtained from encoder\n",
    "            pool_indices (list): Pool indices Pytorch tensors in order the pooling modules in the encoder\n",
    "        Returns:\n",
    "            x (Tensor): decoded image tensor\n",
    "        '''\n",
    "        x_current = x\n",
    "\n",
    "        k_pool = 0\n",
    "        reversed_pool_indices = list(reversed(pool_indices))\n",
    "        for module_decode in self.decoder:\n",
    "\n",
    "            # If the module is unpooling, collect the appropriate pooling indices\n",
    "            if isinstance(module_decode, nn.MaxUnpool2d):\n",
    "                x_current = module_decode(x_current, indices=reversed_pool_indices[k_pool])\n",
    "                k_pool += 1\n",
    "            else:\n",
    "                x_current = module_decode(x_current)\n",
    "\n",
    "        return x_current"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-Encoder\n",
    "uitleggen dat geheel process combinatie is van beiden en dat dit de Auto-Encoder voorstelt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoderVGG(nn.Module):\n",
    "    '''Auto-Encoder based on the VGG-16 with batch normalization template model. The class is comprised of\n",
    "    an encoder and a decoder.\n",
    "    Args:\n",
    "        pretrained_params (bool, optional): If the network should be populated with pre-trained VGG parameters.\n",
    "            Defaults to True.\n",
    "    '''\n",
    "    channels_in = EncoderVGG.channels_in\n",
    "    channels_code = EncoderVGG.channels_code\n",
    "    channels_out = DecoderVGG.channels_out\n",
    "\n",
    "    def __init__(self, pretrained_params=True):\n",
    "        super(AutoEncoderVGG, self).__init__()\n",
    "\n",
    "        self.encoder = EncoderVGG(pretrained_params=pretrained_params)\n",
    "        self.decoder = DecoderVGG(self.encoder.encoder)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''Forward the autoencoder for image input\n",
    "        Args:\n",
    "            x (Tensor): image tensor\n",
    "        Returns:\n",
    "            x_prime (Tensor): image tensor following encoding and decoding\n",
    "        '''\n",
    "        code, pool_indices = self.encoder(x)\n",
    "        x_prime = self.decoder(code, pool_indices)\n",
    "\n",
    "        return x_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merger layer in Encoder\n",
    "make data vector along one dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features of Auto-encoder as starting point for Clustering\n",
    "recurring higher-level features of the image dataset in a lower dimension\n",
    "The Encoder beschikt over representatieve weergave van meest voorkomende vormen en achtergronden van \n",
    "gerechten."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b15ee443de15a7c7a9e59449ab0d06bb25873493c1d52931efe00f2e6ab94104"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
