{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCNN clustering techniques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In sprint2 we tried a lot of different feature extraction techniques like SIFT, HOG, SURF to improve the clustering of pictures of different dishes. However the result was not as hoped and the cluster were very poor.\n",
    "\n",
    "We will try to improve this with useing Deep Convolutionals Neural Networks to try to dynamically learn the specific features and improve the clustering performance.\n",
    "\n",
    "source: https://towardsdatascience.com/image-clustering-implementation-with-pytorch-587af1d14123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## auto-encoding\n",
    "TODO: steps of auto encoding in eigen woorden uitleggen en auto encoding uitleggen\n",
    "\n",
    "We will create the Encoder\n",
    "\n",
    "Uitleggen waarom 2 lagen verwijderd zijn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "\n",
    "class EncoderVGG(nn.Module):\n",
    "    '''Encoder of image based on the architecture of VGG-16 with batch normalization.\n",
    "    Args:\n",
    "        pretrained_params (bool, optional): If the network should be populated with pre-trained VGG parameters.\n",
    "            Defaults to True.\n",
    "    '''\n",
    "    channels_in = 3\n",
    "    channels_code = 512\n",
    "\n",
    "    def __init__(self, pretrained_params=True):\n",
    "        super(EncoderVGG, self).__init__()\n",
    "\n",
    "        vgg = models.vgg16_bn(pretrained=pretrained_params)\n",
    "        del vgg.classifier\n",
    "        del vgg.avgpool\n",
    "\n",
    "        self.encoder = self._encodify_(vgg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pooling indices\n",
    "uitleg _encodify zetten en indices van max-pooling krijgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderVGG(EncoderVGG):\n",
    "    def _encodify_(self, encoder):\n",
    "        '''Create list of modules for encoder based on the architecture in VGG template model.\n",
    "        In the encoder-decoder architecture, the unpooling operations in the decoder require pooling\n",
    "        indices from the corresponding pooling operation in the encoder. In VGG template, these indices\n",
    "        are not returned. Hence the need for this method to extent the pooling operations.\n",
    "        Args:\n",
    "            encoder : the template VGG model\n",
    "        Returns:\n",
    "            modules : the list of modules that define the encoder corresponding to the VGG model\n",
    "        '''\n",
    "        modules = nn.ModuleList()\n",
    "        for module in encoder.features:\n",
    "            if isinstance(module, nn.MaxPool2d):\n",
    "                module_add = nn.MaxPool2d(kernel_size=module.kernel_size,\n",
    "                                            stride=module.stride,\n",
    "                                            padding=module.padding,\n",
    "                                            return_indices=True)\n",
    "                modules.append(module_add)\n",
    "            else:\n",
    "                modules.append(module)\n",
    "\n",
    "        return modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward mini-batch through encoder of _encodify\n",
    "uitleg hoe pool indices in forward worden opgeslagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderVGG(EncoderVGG):\n",
    "    def forward(self, x):\n",
    "        '''Execute the encoder on the image input\n",
    "        Args:\n",
    "            x (Tensor): image tensor\n",
    "        Returns:\n",
    "            x_code (Tensor): code tensor\n",
    "            pool_indices (list): Pool indices tensors in order of the pooling modules\n",
    "        '''\n",
    "        pool_indices = []\n",
    "        x_current = x\n",
    "        for module_encode in self.encoder:\n",
    "            output = module_encode(x_current)\n",
    "\n",
    "            # If the module is pooling, there are two outputs, the second the pool indices\n",
    "            if isinstance(output, tuple) and len(output) == 2:\n",
    "                x_current = output[0]\n",
    "                pool_indices.append(output[1])\n",
    "            else:\n",
    "                x_current = output\n",
    "\n",
    "        return x_current, pool_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "uitleggen dat dit eigenlijk zelfde structuur als encoder is maar dan getransponeerd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderVGG(nn.Module):\n",
    "    '''Decoder of code based on the architecture of VGG-16 with batch normalization.\n",
    "    Args:\n",
    "        encoder: The encoder instance of `EncoderVGG` that is to be inverted into a decoder\n",
    "    '''\n",
    "    channels_in = EncoderVGG.channels_code\n",
    "    channels_out = 3\n",
    "\n",
    "    def __init__(self, encoder):\n",
    "        super(DecoderVGG, self).__init__()\n",
    "\n",
    "        self.decoder = self._invert_(encoder)\n",
    "        \n",
    "    def _invert_(self, encoder):\n",
    "        '''Invert the encoder in order to create the decoder as a (more or less) mirror image of the encoder\n",
    "        The decoder is comprised of two principal types: the 2D transpose convolution and the 2D unpooling. The 2D transpose\n",
    "        convolution is followed by batch normalization and activation. Therefore as the module list of the encoder\n",
    "        is iterated over in reverse, a convolution in encoder is turned into transposed convolution plus normalization\n",
    "        and activation, and a maxpooling in encoder is turned into unpooling.\n",
    "        Args:\n",
    "            encoder (ModuleList): the encoder\n",
    "        Returns:\n",
    "            decoder (ModuleList): the decoder obtained by \"inversion\" of encoder\n",
    "        '''\n",
    "        modules_transpose = []\n",
    "        for module in reversed(encoder):\n",
    "\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                kwargs = {'in_channels' : module.out_channels, 'out_channels' : module.in_channels,\n",
    "                          'kernel_size' : module.kernel_size, 'stride' : module.stride,\n",
    "                          'padding' : module.padding}\n",
    "                module_transpose = nn.ConvTranspose2d(**kwargs)\n",
    "                module_norm = nn.BatchNorm2d(module.in_channels)\n",
    "                module_act = nn.ReLU(inplace=True)\n",
    "                modules_transpose += [module_transpose, module_norm, module_act]\n",
    "\n",
    "            elif isinstance(module, nn.MaxPool2d):\n",
    "                kwargs = {'kernel_size' : module.kernel_size, 'stride' : module.stride,\n",
    "                          'padding' : module.padding}\n",
    "                module_transpose = nn.MaxUnpool2d(**kwargs)\n",
    "                modules_transpose += [module_transpose]\n",
    "\n",
    "        # Discard the final normalization and activation, so final module is convolution with bias\n",
    "        modules_transpose = modules_transpose[:-2]\n",
    "\n",
    "        return nn.ModuleList(modules_transpose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## forward through decoder\n",
    "\n",
    "uitleggen adhv eerste afbeelding dat alles doorlopen is \n",
    "Ook \"Code\" van figuur extra uitleggen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderVGG(DecoderVGG):  \n",
    "    def forward(self, x, pool_indices):\n",
    "        '''Execute the decoder on the code tensor input\n",
    "        Args:\n",
    "            x (Tensor): code tensor obtained from encoder\n",
    "            pool_indices (list): Pool indices Pytorch tensors in order the pooling modules in the encoder\n",
    "        Returns:\n",
    "            x (Tensor): decoded image tensor\n",
    "        '''\n",
    "        x_current = x\n",
    "\n",
    "        k_pool = 0\n",
    "        reversed_pool_indices = list(reversed(pool_indices))\n",
    "        for module_decode in self.decoder:\n",
    "\n",
    "            # If the module is unpooling, collect the appropriate pooling indices\n",
    "            if isinstance(module_decode, nn.MaxUnpool2d):\n",
    "                x_current = module_decode(x_current, indices=reversed_pool_indices[k_pool])\n",
    "                k_pool += 1\n",
    "            else:\n",
    "                x_current = module_decode(x_current)\n",
    "\n",
    "        return x_current"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-Encoder\n",
    "uitleggen dat geheel process combinatie is van beiden en dat dit de Auto-Encoder voorstelt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderVGG(nn.Module):\n",
    "    '''Encoder of image based on the architecture of VGG-16 with batch normalization.\n",
    "    Args:\n",
    "        pretrained_params (bool, optional): If the network should be populated with pre-trained VGG parameters.\n",
    "            Defaults to True.\n",
    "    '''\n",
    "    channels_in = 3\n",
    "    channels_code = 512\n",
    "\n",
    "    def __init__(self, pretrained_params=True):\n",
    "        super(EncoderVGG, self).__init__()\n",
    "\n",
    "        vgg = models.vgg16_bn(pretrained=pretrained_params)\n",
    "        del vgg.classifier\n",
    "        del vgg.avgpool\n",
    "\n",
    "        self.encoder = self._encodify_(vgg)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''Execute the encoder on the image input\n",
    "        Args:\n",
    "            x (Tensor): image tensor\n",
    "        Returns:\n",
    "            x_code (Tensor): code tensor\n",
    "            pool_indices (list): Pool indices tensors in order of the pooling modules\n",
    "        '''\n",
    "        pool_indices = []\n",
    "        x_current = x\n",
    "        for module_encode in self.encoder:\n",
    "            output = module_encode(x_current)\n",
    "\n",
    "            # If the module is pooling, there are two outputs, the second the pool indices\n",
    "            if isinstance(output, tuple) and len(output) == 2:\n",
    "                x_current = output[0]\n",
    "                pool_indices.append(output[1])\n",
    "            else:\n",
    "                x_current = output\n",
    "\n",
    "        return x_current, pool_indices\n",
    "\n",
    "    @staticmethod\n",
    "    def dim_code(img_dim):\n",
    "        '''Convenience function to provide dimension of code given a square image of specified size. The transformation\n",
    "        is defined by the details of the VGG method. The aim should be to resize the image to produce an integer\n",
    "        code dimension.\n",
    "        Args:\n",
    "            img_dim (int): Height/width dimension of the tentative square image to input to the auto-encoder\n",
    "        Returns:\n",
    "            code_dim (float): Height/width dimension of the code\n",
    "            int_value (bool): If False, the tentative image dimension will not produce an integer dimension for the\n",
    "                code. If True it will. For actual applications, this value should be True.\n",
    "        '''\n",
    "        value = img_dim / 2**5\n",
    "        int_value = img_dim % 2**5 == 0\n",
    "        return value, int_value\n",
    "\n",
    "    def _encodify_(self, encoder):\n",
    "        '''Create list of modules for encoder based on the architecture in VGG template model.\n",
    "        In the encoder-decoder architecture, the unpooling operations in the decoder require pooling\n",
    "        indices from the corresponding pooling operation in the encoder. In VGG template, these indices\n",
    "        are not returned. Hence the need for this method to extent the pooling operations.\n",
    "        Args:\n",
    "            encoder : the template VGG model\n",
    "        Returns:\n",
    "            modules : the list of modules that define the encoder corresponding to the VGG model\n",
    "        '''\n",
    "        modules = nn.ModuleList()\n",
    "        for module in encoder.features:\n",
    "            if isinstance(module, nn.MaxPool2d):\n",
    "                module_add = nn.MaxPool2d(kernel_size=module.kernel_size,\n",
    "                                          stride=module.stride,\n",
    "                                          padding=module.padding,\n",
    "                                          return_indices=True)\n",
    "                modules.append(module_add)\n",
    "            else:\n",
    "                modules.append(module)\n",
    "\n",
    "        return modules\n",
    "\n",
    "\n",
    "class DecoderVGG(nn.Module):\n",
    "    '''Decoder of code based on the architecture of VGG-16 with batch normalization.\n",
    "    The decoder is created from a pseudo-inversion of the encoder based on VGG-16 with batch normalization. The\n",
    "    pesudo-inversion is obtained by (1) replacing max pooling layers in the encoder with max un-pooling layers with\n",
    "    pooling indices from the mirror image max pooling layer, and by (2) replacing 2D convolutions with transposed\n",
    "    2D convolutions. The ReLU and batch normalization layers are the same as in the encoder, that is subsequent to\n",
    "    the convolution layer.\n",
    "    Args:\n",
    "        encoder: The encoder instance of `EncoderVGG` that is to be inverted into a decoder\n",
    "    '''\n",
    "    channels_in = EncoderVGG.channels_code\n",
    "    channels_out = 3\n",
    "\n",
    "    def __init__(self, encoder):\n",
    "        super(DecoderVGG, self).__init__()\n",
    "\n",
    "        self.decoder = self._invert_(encoder)\n",
    "\n",
    "    def forward(self, x, pool_indices):\n",
    "        '''Execute the decoder on the code tensor input\n",
    "        Args:\n",
    "            x (Tensor): code tensor obtained from encoder\n",
    "            pool_indices (list): Pool indices Pytorch tensors in order the pooling modules in the encoder\n",
    "        Returns:\n",
    "            x (Tensor): decoded image tensor\n",
    "        '''\n",
    "        x_current = x\n",
    "\n",
    "        k_pool = 0\n",
    "        reversed_pool_indices = list(reversed(pool_indices))\n",
    "        for module_decode in self.decoder:\n",
    "\n",
    "            # If the module is unpooling, collect the appropriate pooling indices\n",
    "            if isinstance(module_decode, nn.MaxUnpool2d):\n",
    "                x_current = module_decode(x_current, indices=reversed_pool_indices[k_pool])\n",
    "                k_pool += 1\n",
    "            else:\n",
    "                x_current = module_decode(x_current)\n",
    "\n",
    "        return x_current\n",
    "\n",
    "    def _invert_(self, encoder):\n",
    "        '''Invert the encoder in order to create the decoder as a (more or less) mirror image of the encoder\n",
    "        The decoder is comprised of two principal types: the 2D transpose convolution and the 2D unpooling. The 2D transpose\n",
    "        convolution is followed by batch normalization and activation. Therefore as the module list of the encoder\n",
    "        is iterated over in reverse, a convolution in encoder is turned into transposed convolution plus normalization\n",
    "        and activation, and a maxpooling in encoder is turned into unpooling.\n",
    "        Args:\n",
    "            encoder (ModuleList): the encoder\n",
    "        Returns:\n",
    "            decoder (ModuleList): the decoder obtained by \"inversion\" of encoder\n",
    "        '''\n",
    "        modules_transpose = []\n",
    "        for module in reversed(encoder):\n",
    "\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                kwargs = {'in_channels' : module.out_channels, 'out_channels' : module.in_channels,\n",
    "                          'kernel_size' : module.kernel_size, 'stride' : module.stride,\n",
    "                          'padding' : module.padding}\n",
    "                module_transpose = nn.ConvTranspose2d(**kwargs)\n",
    "                module_norm = nn.BatchNorm2d(module.in_channels)\n",
    "                module_act = nn.ReLU(inplace=True)\n",
    "                modules_transpose += [module_transpose, module_norm, module_act]\n",
    "\n",
    "            elif isinstance(module, nn.MaxPool2d):\n",
    "                kwargs = {'kernel_size' : module.kernel_size, 'stride' : module.stride,\n",
    "                          'padding' : module.padding}\n",
    "                module_transpose = nn.MaxUnpool2d(**kwargs)\n",
    "                modules_transpose += [module_transpose]\n",
    "\n",
    "        # Discard the final normalization and activation, so final module is convolution with bias\n",
    "        modules_transpose = modules_transpose[:-2]\n",
    "\n",
    "        return nn.ModuleList(modules_transpose)\n",
    "\n",
    "\n",
    "class AutoEncoderVGG(nn.Module):\n",
    "    '''Auto-Encoder based on the VGG-16 with batch normalization template model. The class is comprised of\n",
    "    an encoder and a decoder.\n",
    "    Args:\n",
    "        pretrained_params (bool, optional): If the network should be populated with pre-trained VGG parameters.\n",
    "            Defaults to True.\n",
    "    '''\n",
    "    channels_in = EncoderVGG.channels_in\n",
    "    channels_code = EncoderVGG.channels_code\n",
    "    channels_out = DecoderVGG.channels_out\n",
    "\n",
    "    def __init__(self, pretrained_params=True):\n",
    "        super(AutoEncoderVGG, self).__init__()\n",
    "\n",
    "        self.encoder = EncoderVGG(pretrained_params=pretrained_params)\n",
    "        self.decoder = DecoderVGG(self.encoder.encoder)\n",
    "\n",
    "    @staticmethod\n",
    "    def dim_code(img_dim):\n",
    "        '''Convenience function to provide dimension of code given a square image of specified size. The transformation\n",
    "        is defined by the details of the VGG method. The aim should be to resize the image to produce an integer\n",
    "        code dimension.\n",
    "        Args:\n",
    "            img_dim (int): Height/width dimension of the tentative square image to input to the auto-encoder\n",
    "        Returns:\n",
    "            code_dim (float): Height/width dimension of the code\n",
    "            int_value (bool): If False, the tentative image dimension will not produce an integer dimension for the\n",
    "                code. If True it will. For actual applications, this value should be True.\n",
    "        '''\n",
    "        return EncoderVGG.dim_code(img_dim)\n",
    "\n",
    "    @staticmethod\n",
    "    def state_dict_mutate(encoder_or_decoder, ae_state_dict):\n",
    "        '''Mutate an auto-encoder state dictionary into a pure encoder or decoder state dictionary\n",
    "        The method depends on the naming of the encoder and decoder attribute names as defined in the auto-encoder\n",
    "        initialization. Currently these names are \"encoder\" and \"decoder\".\n",
    "        The state dictionary that is returned can be loaded into a pure EncoderVGG or DecoderVGG instance.\n",
    "        Args:\n",
    "            encoder_or_decoder (str): Specification if mutation should be to an encoder state dictionary or decoder\n",
    "                state dictionary, where the former is denoted with \"encoder\" and the latter \"decoder\"\n",
    "            ae_state_dict (OrderedDict): The auto-encoder state dictionary to mutate\n",
    "        Returns:\n",
    "            state_dict (OrderedDict): The mutated state dictionary that can be loaded into either an EncoderVGG\n",
    "                or DecoderVGG instance\n",
    "        Raises:\n",
    "            RuntimeError : if state dictionary contains keys that cannot be attributed to either encoder or decoder\n",
    "            ValueError : if specified mutation is neither \"encoder\" or \"decoder\"\n",
    "        '''\n",
    "        if not (encoder_or_decoder == 'encoder' or encoder_or_decoder == 'decoder'):\n",
    "            raise ValueError('State dictionary mutation only for \"encoder\" or \"decoder\", not {}'.format(encoder_or_decoder))\n",
    "\n",
    "        keys = list(ae_state_dict)\n",
    "        for key in keys:\n",
    "            if 'encoder' in key or 'decoder' in key:\n",
    "                if encoder_or_decoder in key:\n",
    "                    key_new = key[len(encoder_or_decoder) + 1:]\n",
    "                    ae_state_dict[key_new] = ae_state_dict[key]\n",
    "                    del ae_state_dict[key]\n",
    "\n",
    "                else:\n",
    "                    del ae_state_dict[key]\n",
    "\n",
    "            else:\n",
    "                raise RuntimeError('State dictionary key {} is neither part of encoder or decoder'.format(key))\n",
    "\n",
    "        return ae_state_dict\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''Forward the autoencoder for image input\n",
    "        Args:\n",
    "            x (Tensor): image tensor\n",
    "        Returns:\n",
    "            x_prime (Tensor): image tensor following encoding and decoding\n",
    "        '''\n",
    "        code, pool_indices = self.encoder(x)\n",
    "        x_prime = self.decoder(code, pool_indices)\n",
    "\n",
    "        return x_prime\n",
    "\n",
    "\n",
    "class EncoderVGGMerged(EncoderVGG):\n",
    "    '''Special case of the VGG Encoder wherein the code is merged along the height/width dimension. This is a thin child\n",
    "    class of `EncoderVGG`.\n",
    "    Args:\n",
    "        merger_type (str, optional): Defines how the code is merged. If `None`, there is no merger and the identical\n",
    "            functionality to the parent class `EncoderVGG` is obtained. If \"mean\", the channels for the height/width\n",
    "            code cells are averaged; the number of channels are identical between input and output. If \"flatten\", the\n",
    "            channels for the height/width code cells are stacked on top each other; the number of channels of the output\n",
    "            is the number of channels of the input multiplied by number of height cells and multiplied by the number\n",
    "            of width cells.\n",
    "        pretrained_params (bool, optional): If the network should be populated with pre-trained VGG parameters.\n",
    "            Defaults to True.\n",
    "    '''\n",
    "    def __init__(self, merger_type=None, pretrained_params=True):\n",
    "        super(EncoderVGGMerged, self).__init__(pretrained_params=pretrained_params)\n",
    "\n",
    "        if merger_type is None:\n",
    "            self.code_post_process = lambda x: x\n",
    "            self.code_post_process_kwargs = {}\n",
    "        elif merger_type == 'mean':\n",
    "            self.code_post_process = torch.mean\n",
    "            self.code_post_process_kwargs = {'dim' : (-2, -1)}\n",
    "        elif merger_type == 'flatten':\n",
    "            self.code_post_process = torch.flatten\n",
    "            self.code_post_process_kwargs = {'start_dim' : 1, 'end_dim' : -1}\n",
    "        else:\n",
    "            raise ValueError('Unknown merger type for the encoder code: {}'.format(merger_type))\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''Execute the encoder on the image input\n",
    "        Args:\n",
    "            x (Tensor): image tensor\n",
    "        Returns:\n",
    "            x_code (Tensor): merged code tensor\n",
    "        '''\n",
    "        x_current, _ = super().forward(x)\n",
    "        x_code = self.code_post_process(x_current, **self.code_post_process_kwargs)\n",
    "\n",
    "        return x_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoderVGG(nn.Module):\n",
    "    '''Auto-Encoder based on the VGG-16 with batch normalization template model. The class is comprised of\n",
    "    an encoder and a decoder.\n",
    "    Args:\n",
    "        pretrained_params (bool, optional): If the network should be populated with pre-trained VGG parameters.\n",
    "            Defaults to True.\n",
    "    '''\n",
    "    channels_in = EncoderVGG.channels_in\n",
    "    channels_code = EncoderVGG.channels_code\n",
    "    channels_out = DecoderVGG.channels_out\n",
    "\n",
    "    def __init__(self, pretrained_params=True):\n",
    "        super(AutoEncoderVGG, self).__init__()\n",
    "\n",
    "        self.encoder = EncoderVGG(pretrained_params=pretrained_params)\n",
    "        self.decoder = DecoderVGG(self.encoder.encoder)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''Forward the autoencoder for image input\n",
    "        Args:\n",
    "            x (Tensor): image tensor\n",
    "        Returns:\n",
    "            x_prime (Tensor): image tensor following encoding and decoding\n",
    "        '''\n",
    "        code, pool_indices = self.encoder(x)\n",
    "        x_prime = self.decoder(code, pool_indices)\n",
    "\n",
    "        return x_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merger layer in Encoder\n",
    "make data vector along one dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderVGGMerged(EncoderVGG):\n",
    "    '''Special case of the VGG Encoder wherein the code is merged along the height/width dimension. This is a thin child\n",
    "    class of `EncoderVGG`.\n",
    "    Args:\n",
    "        merger_type (str, optional): Defines how the code is merged. \n",
    "        \n",
    "    '''\n",
    "    def __init__(self, merger_type='mean', pretrained_params=True):\n",
    "        super(EncoderVGGMerged, self).__init__(pretrained_params=pretrained_params)\n",
    "\n",
    "        if merger_type is None:\n",
    "            self.code_post_process = lambda x: x\n",
    "            self.code_post_process_kwargs = {}\n",
    "        elif merger_type == 'mean':\n",
    "            self.code_post_process = torch.mean\n",
    "            self.code_post_process_kwargs = {'dim' : (-2, -1)}\n",
    "        elif merger_type == 'flatten':\n",
    "            self.code_post_process = torch.flatten\n",
    "            self.code_post_process_kwargs = {'start_dim' : 1, 'end_dim' : -1}\n",
    "        else:\n",
    "            raise ValueError('Unknown merger type for the encoder code: {}'.format(merger_type))\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''Execute the encoder on the image input\n",
    "        Args:\n",
    "            x (Tensor): image tensor\n",
    "        Returns:\n",
    "            x_code (Tensor): merged code tensor\n",
    "        '''\n",
    "        x_current, _ = super().forward(x)\n",
    "        x_code = self.code_post_process(x_current, **self.code_post_process_kwargs)\n",
    "\n",
    "        return x_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Auto-Encoder Learner for the fungi dataset, a child of `_Learner`\n",
    "Written by: Anders Ohrn, October 2020\n",
    "'''\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from _learner import _Learner, progress_bar\n",
    "\n",
    "class AELearner(_Learner):\n",
    "    '''Auto-encoder Learner class applied to the fungi image dataset for learning efficient encoding and decoding\n",
    "    Args:\n",
    "        To be written\n",
    "    '''\n",
    "    def __init__(self, run_label='', random_seed=None, f_out=sys.stdout,\n",
    "                       raw_csv_toc=None, raw_csv_root=None,\n",
    "                       save_tmp_name='model_in_training',\n",
    "                       selector=None, iselector=None,\n",
    "                       dataset_type='full basic',\n",
    "                       loader_batch_size=16, num_workers=0,\n",
    "                       show_batch_progress=True, deterministic=True,\n",
    "                       lr_init=0.01, momentum=0.9,\n",
    "                       scheduler_step_size=15, scheduler_gamma=0.1,\n",
    "                       freeze_encoder=False,\n",
    "                       img_input_dim=224, img_n_splits=6, crop_step_size=32, crop_dim=64,\n",
    "                       square=True):\n",
    "\n",
    "        dataset_kwargs = {'img_input_dim': img_input_dim, 'img_n_splits': img_n_splits,\n",
    "                          'crop_step_size': crop_step_size, 'crop_dim': crop_dim, 'square': square}\n",
    "\n",
    "        super(AELearner, self).__init__(run_label=run_label, random_seed=random_seed, f_out=f_out,\n",
    "                                        raw_csv_toc=raw_csv_toc, raw_csv_root=raw_csv_root,\n",
    "                                        save_tmp_name=save_tmp_name,\n",
    "                                        selector=selector, iselector=iselector,\n",
    "                                        dataset_type=dataset_type, dataset_kwargs=dataset_kwargs,\n",
    "                                        loader_batch_size=loader_batch_size, num_workers=num_workers,\n",
    "                                        show_batch_progress=show_batch_progress,\n",
    "                                        deterministic=deterministic)\n",
    "\n",
    "        self.inp_freeze_encoder = freeze_encoder\n",
    "        self.inp_lr_init = lr_init\n",
    "        self.inp_momentum = momentum\n",
    "        self.inp_scheduler_step_size = scheduler_step_size\n",
    "        self.inp_scheduler_gamma = scheduler_gamma\n",
    "\n",
    "        self.model = AutoEncoderVGG()\n",
    "        self.criterion = nn.MSELoss()\n",
    "        if self.inp_freeze_encoder:\n",
    "            self.set_sgd_optim(lr=self.inp_lr_init,\n",
    "                               scheduler_step_size=self.inp_scheduler_step_size,\n",
    "                               scheduler_gamma=self.inp_scheduler_gamma,\n",
    "                               parameters=self.model.decoder.parameters())\n",
    "        else:\n",
    "            self.set_sgd_optim(lr=self.inp_lr_init,\n",
    "                               scheduler_step_size=self.inp_scheduler_step_size,\n",
    "                               scheduler_gamma=self.inp_scheduler_gamma,\n",
    "                               parameters=self.model.parameters())\n",
    "\n",
    "        self.print_inp()\n",
    "\n",
    "    def load_model(self, model_path):\n",
    "        '''Load auto-encoder from saved state dictionary\n",
    "        Args:\n",
    "            model_path (str): Path to the saved model to load\n",
    "        '''\n",
    "        saved_dict = torch.load('{}.tar'.format(model_path))\n",
    "        self.model.load_state_dict(saved_dict[self.STATE_KEY_SAVE])\n",
    "\n",
    "    def save_model(self, model_path):\n",
    "        '''Save encoder state dictionary\n",
    "        Args:\n",
    "            model_path (str): Path and name to file to save state dictionary to. The filename on disk is this argument\n",
    "                appended with suffix `.tar`\n",
    "        '''\n",
    "        torch.save({self.STATE_KEY_SAVE: self.model.state_dict()},\n",
    "                   '{}.tar'.format(model_path))\n",
    "\n",
    "    def train(self, n_epochs):\n",
    "        '''Train model for set number of epochs\n",
    "        Args:\n",
    "            n_epochs (int): Number of epochs to train the model for\n",
    "        '''\n",
    "        self.model.train()\n",
    "        for epoch in range(n_epochs):\n",
    "            print('Epoch {}/{}...'.format(epoch, n_epochs - 1), file=self.inp_f_out)\n",
    "\n",
    "            running_loss = 0.0\n",
    "            n_instances = 0\n",
    "            for inputs in self.dataloader:\n",
    "                size_batch = inputs[self.dataset.returnkey.image].size(0)\n",
    "                image = inputs[self.dataset.returnkey.image].to(self.device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # Compute loss\n",
    "                output = self.model(image)\n",
    "                loss = self.criterion(output, image)\n",
    "\n",
    "                # Back-propagate and optimize\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.lr_scheduler.step()\n",
    "\n",
    "                # Update aggregates and reporting\n",
    "                running_loss += loss.item() * size_batch\n",
    "                if self.inp_show_batch_progress:\n",
    "                    n_instances += size_batch\n",
    "                    progress_bar(n_instances, self.dataset_size)\n",
    "\n",
    "            running_loss = running_loss / self.dataset_size\n",
    "            print('\\nLoss: {:.4f}'.format(running_loss), file=self.inp_f_out)\n",
    "\n",
    "            self.save_model(self.inp_save_tmp_name)\n",
    "\n",
    "    def eval(self, dloader=None, untransform=None):\n",
    "        '''Generator to evaluate the Auto-encoder for a selection of images\n",
    "        Args:\n",
    "            dloader (optional): Dataloader to collect data with. Defaults to `None`, in which case the Dataloader of\n",
    "                `self` is used.\n",
    "            untransform (optional): Image transform to apply to the model output. Typically a de-normalizing transform\n",
    "                to make image human readable\n",
    "        Yields:\n",
    "            img_batch (PyTorch Tensor): batch of images following evaluation\n",
    "        '''\n",
    "        self.model.eval()\n",
    "        if dloader is None:\n",
    "            dloader = self.dataloader\n",
    "\n",
    "        ret_batch = []\n",
    "        for inputs in dloader:\n",
    "            image = inputs[self.dataset.returnkey.image].to(self.device)\n",
    "            output = self.model(image)\n",
    "            for img in output:\n",
    "                img = img.detach()\n",
    "                if not untransform is None:\n",
    "                    img = untransform(img)\n",
    "                ret_batch.append(img)\n",
    "\n",
    "            yield torch.stack(ret_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hikmat/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/hikmat/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_BN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run at 2022/12/4 19:20:48 with arguments:\n",
      "run_label : test\n",
      "random_seed : 453\n",
      "f_out : <ipykernel.iostream.OutStream object at 0x7f105f68ec80>\n",
      "raw_csv_toc : None\n",
      "raw_csv_root : None\n",
      "save_tmp_name : model_in_training\n",
      "selector : None\n",
      "iselector : None\n",
      "dataset_type : full basic\n",
      "dataset_kwargs : {'img_input_dim': 224, 'img_n_splits': 6, 'crop_step_size': 32, 'crop_dim': 64, 'square': True}\n",
      "loader_batch_size : 16\n",
      "num_workers : 0\n",
      "show_batch_progress : True\n",
      "deterministic : True\n",
      "epoch_conclude_func : <function _Learner.__init__.<locals>.<lambda> at 0x7f0f7afe0f70>\n",
      "freeze_encoder : False\n",
      "lr_init : 0.01\n",
      "momentum : 0.9\n",
      "scheduler_step_size : 15\n",
      "scheduler_gamma : 0.1\n",
      "Epoch 0/1...\n",
      "processed image\n",
      "processed image\n",
      "processed image\n",
      "processed image\n",
      "processed image\n",
      "processed image\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'FoodDataset' object has no attribute 'returnkey'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m ae \u001b[39m=\u001b[39m AELearner(run_label\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m, random_seed\u001b[39m=\u001b[39m\u001b[39m453\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m ae\u001b[39m.\u001b[39;49mtrain(\u001b[39m2\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn [18], line 91\u001b[0m, in \u001b[0;36mAELearner.train\u001b[0;34m(self, n_epochs)\u001b[0m\n\u001b[1;32m     89\u001b[0m n_instances \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     90\u001b[0m \u001b[39mfor\u001b[39;00m inputs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataloader:\n\u001b[0;32m---> 91\u001b[0m     size_batch \u001b[39m=\u001b[39m inputs[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset\u001b[39m.\u001b[39;49mreturnkey\u001b[39m.\u001b[39mimage]\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[1;32m     92\u001b[0m     image \u001b[39m=\u001b[39m inputs[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39mreturnkey\u001b[39m.\u001b[39mimage]\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     94\u001b[0m     \u001b[39m# zero the parameter gradients\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FoodDataset' object has no attribute 'returnkey'"
     ]
    }
   ],
   "source": [
    "ae = AELearner(run_label=\"test\", random_seed=453)\n",
    "ae.train(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features of Auto-encoder as starting point for Clustering\n",
    "recurring higher-level features of the image dataset in a lower dimension\n",
    "The Encoder beschikt over representatieve weergave van meest voorkomende vormen en achtergronden van \n",
    "gerechten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.int32'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.int_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
