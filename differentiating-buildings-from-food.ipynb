{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-11-14T17:33:51.878412Z","iopub.status.busy":"2022-11-14T17:33:51.877790Z","iopub.status.idle":"2022-11-14T17:33:56.220956Z","shell.execute_reply":"2022-11-14T17:33:56.219397Z","shell.execute_reply.started":"2022-11-14T17:33:51.878302Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","from fastai.imports import *\n","from fastai.vision.all import *\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","# for dirname, _, filenames in os.walk('/kaggle/input'):\n","#     for filename in filenames:\n","#         print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","metadata":{},"source":["# analysing clustering,dimensionality reduction and feature extraction techniques with labeled data"]},{"cell_type":"markdown","metadata":{},"source":["in this notebook we use a subset of the [Food-101 dataset](https://www.kaggle.com/datasets/kmader/food41) and the [House Rooms & Streets Image Dataset](https://www.kaggle.com/datasets/mikhailma/house-rooms-streets-image-dataset)"]},{"cell_type":"markdown","metadata":{},"source":["first look at some sample images from our datasets"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-11-14T17:33:56.225156Z","iopub.status.busy":"2022-11-14T17:33:56.223734Z","iopub.status.idle":"2022-11-14T17:33:56.232191Z","shell.execute_reply":"2022-11-14T17:33:56.230843Z","shell.execute_reply.started":"2022-11-14T17:33:56.225084Z"},"trusted":true},"outputs":[],"source":["food_path=Path(\"tripadvisor_dataset/kaggle/archive (1)/images\")\n","building_path=Path(\"tripadvisor_dataset/kaggle/archive/kaggle_room_street_data\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-14T16:25:04.975645Z","iopub.status.busy":"2022-11-14T16:25:04.975343Z","iopub.status.idle":"2022-11-14T16:25:30.977766Z","shell.execute_reply":"2022-11-14T16:25:30.976667Z","shell.execute_reply.started":"2022-11-14T16:25:04.975620Z"},"trusted":true},"outputs":[],"source":["fig, ax = plt.subplots(figsize=(10,128),nrows=30, ncols=2, ) #make a figure to plot\n","for i,category in enumerate(os.listdir(food_path)): #loop over image categories\n","    for j, img in enumerate(os.listdir(os.path.join(food_path,category))): # loop over images in each category\n","        ax[i,j].imshow(PILImage.create(os.path.join(food_path,category,img)),label=category) #plot image\n","        ax[i,j].set_title(category,fontsize = 14)\n","        if(j==1):break\n","    if i==29:break"]},{"cell_type":"markdown","metadata":{},"source":["these were some food images, now let's look at some non-food images"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-14T16:25:30.981910Z","iopub.status.busy":"2022-11-14T16:25:30.980052Z","iopub.status.idle":"2022-11-14T16:25:32.523771Z","shell.execute_reply":"2022-11-14T16:25:32.522609Z","shell.execute_reply.started":"2022-11-14T16:25:30.981842Z"},"trusted":true},"outputs":[],"source":["fig, ax = plt.subplots(figsize=(10,10),nrows=2, ncols=3, ) #make a figure to plot\n","for i,category in enumerate(os.listdir(building_path)): #loop over street data categories\n","    for j, img in enumerate(os.listdir(os.path.join(building_path,category))): # loop over images in each category\n","        ax[i,j].imshow(PILImage.create(os.path.join(building_path,category,img)),label=category) #plot image\n","        ax[i,j].set_title(category,fontsize = 14)\n","        if(j==2):break\n"]},{"cell_type":"markdown","metadata":{},"source":["we will make our custom dataset based on the street and food data"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-11-14T17:34:41.662431Z","iopub.status.busy":"2022-11-14T17:34:41.661701Z","iopub.status.idle":"2022-11-14T17:34:41.669313Z","shell.execute_reply":"2022-11-14T17:34:41.668266Z","shell.execute_reply.started":"2022-11-14T17:34:41.662391Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["there are 101 food classes\n","there are 2 non-food classes\n"]}],"source":["num_food_class=len(os.listdir(food_path))\n","print(f\"there are {num_food_class} food classes\" )\n","print(f\"there are 2 non-food classes\" )"]},{"cell_type":"markdown","metadata":{},"source":["we will make a dataset consisting of 505 food images (5 images from each class) and 500 non-food images"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-11-14T17:33:56.234394Z","iopub.status.busy":"2022-11-14T17:33:56.233592Z","iopub.status.idle":"2022-11-14T17:33:56.255074Z","shell.execute_reply":"2022-11-14T17:33:56.253830Z","shell.execute_reply.started":"2022-11-14T17:33:56.234356Z"},"trusted":true},"outputs":[],"source":["# Resizing to this sizes\n","IMG_HEIGHT = 128\n","IMG_WIDTH = 128\n","\n","def create_dataset():\n","    # n = amount of images\n","    n=1005\n","    counter=0\n","    images = np.zeros((n, IMG_HEIGHT* IMG_WIDTH* 3))\n","    \n","    for i,category in enumerate(os.listdir(food_path)): #loop over image food categories\n","        for j, img in enumerate(os.listdir(os.path.join(food_path,category))[:5]): # loop over first 5 images in each category\n","            img=PILImage.create(os.path.join(food_path,category,img))#plot image\n","            img_resized=img.resize((IMG_HEIGHT,IMG_WIDTH))\n","            img_np=np.array(img_resized).flatten()\n","            images[counter]=img_np/255\n","            counter+=1\n","    \n","    \n","    for i,category in enumerate(os.listdir(building_path)): #loop over street data categories\n","        for j, img in enumerate(os.listdir(os.path.join(building_path,category))[:250]): # loop over first 250 images in each category       \n","            img=PILImage.create(os.path.join(building_path,category,img))\n","            img_resized=img.resize((IMG_HEIGHT,IMG_WIDTH))\n","            img_np=np.array(img_resized).flatten()\n","            images[counter]=img_np/255\n","            counter+=1    \n","    return images\n","    \n","    \n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-11-14T17:33:56.257946Z","iopub.status.busy":"2022-11-14T17:33:56.257305Z","iopub.status.idle":"2022-11-14T17:34:41.659965Z","shell.execute_reply":"2022-11-14T17:34:41.658972Z","shell.execute_reply.started":"2022-11-14T17:33:56.257906Z"},"trusted":true},"outputs":[],"source":["images = create_dataset()"]},{"cell_type":"markdown","metadata":{},"source":["After reading the images, extract the features with SIFT."]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2022-11-14T18:00:41.369665Z","iopub.status.busy":"2022-11-14T18:00:41.369197Z","iopub.status.idle":"2022-11-14T18:00:41.727378Z","shell.execute_reply":"2022-11-14T18:00:41.725717Z","shell.execute_reply.started":"2022-11-14T18:00:41.369620Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\hikma\\AppData\\Local\\Temp\\ipykernel_5232\\2264410521.py:22: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  return (descriptor_list, np.array(features))\n"]}],"source":["import cv2\n","\n","def sift_features(images):\n","    sift_vectors = {}\n","    descriptor_list = []\n","    features = []\n","    sift = cv2.xfeatures2d.SIFT_create()\n","    for img in images:\n","        img = img.reshape(IMG_HEIGHT,IMG_WIDTH,3)\n","        # SIFT function only accepts images with 8 bit integer values\n","        image8bit = cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX).astype('uint8')\n","\n","        # Convert the training image to gray scale\n","        training_gray = cv2.cvtColor(image8bit, cv2.COLOR_RGB2GRAY)\n","\n","\n","        keypoints, descriptor = sift.detectAndCompute(training_gray, None)\n","        descriptor_list.extend(descriptor)\n","        features.append(keypoints)\n","\n","\n","    return (descriptor_list, np.array(features))\n","\n","\n","descriptors, features = sift_features(images)\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/plain":["(128,)"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["descriptors[0].shape # descriptors is a list"]},{"cell_type":"markdown","metadata":{},"source":["aanpassen\n","\n","We now have an array with a huge number of descriptors. We cannot use all of them to create the model, so we need to cluster them. A rule-of-thumb is to create k centers with k = number of categories * 10."]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/plain":["array([[ 8.86753446, 19.34532925, 82.81163859, ..., 17.70826953,\n","        19.15390505, 18.32388974],\n","       [32.64206009, 37.62145923, 26.47639485, ...,  9.78540773,\n","         8.98969957, 11.39227468],\n","       [66.10705596, 15.01216545,  9.56934307, ...,  9.08029197,\n","         8.81103001, 15.02514193],\n","       ...,\n","       [17.28239437, 16.84647887, 17.23450704, ...,  7.9528169 ,\n","         8.09507042, 10.53028169],\n","       [18.97676056, 21.73873239, 23.86126761, ...,  4.53802817,\n","         5.90211268, 10.16830986],\n","       [ 7.50725514,  9.70374849, 14.68681983, ...,  9.28234583,\n","         8.49032648, 12.22551391]])"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.cluster import KMeans\n","\n","number_of_categories = 10\n","\n","def kmeans(descriptor_list):\n","    k = number_of_categories * 10\n","    kmeans = KMeans(n_clusters = k, n_init=10)\n","    kmeans.fit(descriptor_list)\n","    visual_words = kmeans.cluster_centers_ \n","    return visual_words\n","\n","visual_words = kmeans(descriptors)\n","visual_words"]},{"cell_type":"markdown","metadata":{},"source":["### Making histograms\n"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["# Find the index of the closest central point to the each sift descriptor. \n","# Takes 2 parameters the first one is a sift descriptor and the second one is the array of central points in k means\n","# Returns the index of the closest central point.  \n","from scipy.spatial import distance\n","\n","def find_index(image, center):\n","    count = 0\n","    ind = 0\n","    for i in range(len(center)):\n","        if(i == 0):\n","           count = distance.euclidean(image, center[i]) \n","           #count = L1_dist(image, center[i])\n","        else:\n","            dist = distance.euclidean(image, center[i]) \n","            #dist = L1_dist(image, center[i])\n","            if(dist < count):\n","                ind = i\n","                count = dist\n","    return ind"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["def image_class(features, centers, descriptors):\n","    begin = 0\n","    histogrammen = []\n","    for f in features:\n","        histogram = np.zeros(len(centers))\n","        for i in range(begin, begin + len(f)):\n","            ind = find_index(descriptors[i], centers)\n","            histogram[ind] += 1\n","        histogrammen.append(histogram)\n","        begin += len(f)\n","\n","    return histogrammen\n","\n","\n","image_features = image_class(features, visual_words, descriptors)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1005\n"]},{"data":{"text/plain":["(100,)"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["print(len(image_features)) # amount of images\n","image_features[0].shape # amount of centroids\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-11-14T17:34:53.347844Z","iopub.status.busy":"2022-11-14T17:34:53.347415Z","iopub.status.idle":"2022-11-14T17:34:53.358922Z","shell.execute_reply":"2022-11-14T17:34:53.357463Z","shell.execute_reply.started":"2022-11-14T17:34:53.347810Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(1005, 49152)"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["images.shape\n","##eerste 505 afbeeldingen zijn voedsel, laatste 500 zijn niet voedsel\n"]},{"cell_type":"markdown","metadata":{},"source":["first attempt making a score function for our kmeans"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2022-11-14T17:35:08.769723Z","iopub.status.busy":"2022-11-14T17:35:08.768292Z","iopub.status.idle":"2022-11-14T17:35:44.923255Z","shell.execute_reply":"2022-11-14T17:35:44.921781Z","shell.execute_reply.started":"2022-11-14T17:35:08.769681Z"},"trusted":true},"outputs":[],"source":["from sklearn.cluster import KMeans\n","from sklearn.preprocessing import StandardScaler\n","sc = StandardScaler()\n","raw_scaled = sc.fit_transform(images)\n","\n","kmeans = KMeans(n_clusters=5, random_state=0)\n","pred=kmeans.fit_predict(raw_scaled)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-11-14T17:35:48.486642Z","iopub.status.busy":"2022-11-14T17:35:48.486185Z","iopub.status.idle":"2022-11-14T17:35:48.494409Z","shell.execute_reply":"2022-11-14T17:35:48.493229Z","shell.execute_reply.started":"2022-11-14T17:35:48.486605Z"},"trusted":true},"outputs":[{"data":{"text/plain":["array([1, 2, 3, ..., 2, 2, 1], dtype=int32)"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["pred"]},{"cell_type":"markdown","metadata":{},"source":["Ideally, we'd like some way to try more clusters ans test our performance easily . We could create a function that returns how good our model is, in order to more quickly try out a few different methods. We'll create a score function to do this. Instead of returning the mean absolute error, we'll calculate a measure of impurity -- that is, how much our model creates clusters where the images in a group are each similar to each other, or dissimilar."]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-11-14T17:36:31.239372Z","iopub.status.busy":"2022-11-14T17:36:31.238919Z","iopub.status.idle":"2022-11-14T17:36:31.245221Z","shell.execute_reply":"2022-11-14T17:36:31.243842Z","shell.execute_reply.started":"2022-11-14T17:36:31.239329Z"},"trusted":true},"outputs":[],"source":["num_clusters=np.unique(pred)##will be the amount of clusters"]},{"cell_type":"markdown","metadata":{},"source":["we will explain this step by step for cluster 0"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-11-14T17:36:57.749820Z","iopub.status.busy":"2022-11-14T17:36:57.749382Z","iopub.status.idle":"2022-11-14T17:36:57.759022Z","shell.execute_reply":"2022-11-14T17:36:57.757640Z","shell.execute_reply.started":"2022-11-14T17:36:57.749776Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(array([ 10,  18,  22,  23,  29,  31,  32,  33,  34,  39,  40,  41,  42,\n","         44,  48,  54,  56,  58,  59,  64,  67,  71,  72,  83,  97,  98,\n","        103, 106, 109, 117, 126, 145, 150, 153, 157, 164, 165, 170, 173,\n","        176, 178, 182, 183, 188, 191, 192, 194, 197, 206, 207, 208, 210,\n","        211, 213, 220, 228, 235, 243, 245, 252, 255, 256, 258, 261, 262,\n","        272, 276, 278, 280, 284, 287, 288, 296, 299, 301, 311, 312, 320,\n","        322, 326, 328, 333, 337, 341, 348, 353, 364, 367, 371, 372, 373,\n","        375, 377, 380, 384, 394, 395, 398, 399, 401, 403, 416, 417, 418,\n","        422, 423, 425, 431, 433, 434, 435, 441, 444, 445, 450, 458, 464,\n","        467, 469, 475, 476, 479, 480, 481, 482, 490, 496, 501, 552, 559,\n","        581, 600, 651, 675, 691, 697, 702, 729, 742, 743, 744, 751, 760,\n","        774, 776, 778, 785, 791, 819, 823, 824, 828, 839, 845, 847, 888,\n","        891, 896, 925, 928, 931, 942, 964, 974, 977, 992, 994, 997]),)"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["image_idx=np.where(pred == 0)#indexes of images in this cluster\n","image_idx"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-11-14T17:37:22.197278Z","iopub.status.busy":"2022-11-14T17:37:22.196815Z","iopub.status.idle":"2022-11-14T17:37:22.206700Z","shell.execute_reply":"2022-11-14T17:37:22.205204Z","shell.execute_reply.started":"2022-11-14T17:37:22.197240Z"},"trusted":true},"outputs":[{"data":{"text/plain":["array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True, False, False, False, False, False, False, False,\n","       False, False, False, False, False, False, False, False, False,\n","       False, False, False, False, False, False, False, False, False,\n","       False, False, False, False, False, False, False, False, False,\n","       False, False, False, False, False, False])"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["binary=image_idx[0]<505 #image indexes smaller than 505 are food\n","binary"]},{"cell_type":"markdown","metadata":{},"source":["calculate the impurity of our cluster with the standard deviation\n","\n","We can measure the similarity of images inside a group by taking the standard deviation of the dependent variable. If it's higher, then it means the images are more different to each other."]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2022-11-14T17:37:37.041986Z","iopub.status.busy":"2022-11-14T17:37:37.041412Z","iopub.status.idle":"2022-11-14T17:37:37.050834Z","shell.execute_reply":"2022-11-14T17:37:37.049318Z","shell.execute_reply.started":"2022-11-14T17:37:37.041946Z"},"trusted":true},"outputs":[{"data":{"text/plain":["0.4259177099999599"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["binary.std()"]},{"cell_type":"markdown","metadata":{},"source":["now putting it all together"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2022-11-14T17:38:20.044008Z","iopub.status.busy":"2022-11-14T17:38:20.043449Z","iopub.status.idle":"2022-11-14T17:38:20.052338Z","shell.execute_reply":"2022-11-14T17:38:20.051413Z","shell.execute_reply.started":"2022-11-14T17:38:20.043961Z"},"trusted":true},"outputs":[],"source":["#for each cluster calculate the std\n","\n","def cluster_score(pred):\n","    score=0\n","    num_clusters=np.unique(pred)\n","    for clusternr in num_clusters:\n","        image_idx=np.where(pred == clusternr)#indexes of images in this cluster\n","        binary=image_idx[0]<505\n","        score+=binary.std()\n","    return score/len(num_clusters) #score averaged out over the number of clusters\n","\n","def calculate_score(pred):\n","    num_clusters = np.unique(pred)##will be the amount of clusters\n","    image_idx = np.where(pred == 0)#indexes of images in this cluster\n","    binary = image_idx[0]<505 #image indexes smaller than 505 are food\n","    return cluster_score(pred)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2022-11-14T17:38:33.673397Z","iopub.status.busy":"2022-11-14T17:38:33.672917Z","iopub.status.idle":"2022-11-14T17:38:33.681913Z","shell.execute_reply":"2022-11-14T17:38:33.680874Z","shell.execute_reply.started":"2022-11-14T17:38:33.673362Z"},"trusted":true},"outputs":[{"data":{"text/plain":["0.44348749509083696"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["cluster_score(pred) #higher is bad"]},{"cell_type":"markdown","metadata":{},"source":["Now we try the kmeans on the SIFT features"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"data":{"text/plain":["0.37863715855097313"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["kmeans = KMeans(n_clusters=5, random_state=0)\n","pred=kmeans.fit_predict(image_features)\n","calculate_score(pred)"]},{"cell_type":"markdown","metadata":{},"source":["The score is lower (which means better). The reason because of this is probably because when you flatten raw data like raw_scaled you lose important spacial information. SIFT captures all the important features. From now we fit using the SIFT features because it gives better results."]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"data":{"text/plain":["0.4999938119960642"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.cluster import DBSCAN\n","\n","dbscan = DBSCAN()\n","pred = dbscan.fit_predict(image_features)\n","calculate_score(pred)"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"data":{"text/plain":["0.2952638422721281"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.cluster import AgglomerativeClustering\n","\n","hier = AgglomerativeClustering(n_clusters=5)\n","pred = hier.fit_predict(image_features)\n","calculate_score(pred)"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"data":{"text/plain":["0.2727707236328636"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.cluster import Birch\n","\n","birch = Birch(n_clusters=5)\n","pred = birch.fit_predict(image_features)\n","calculate_score(pred)"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.2 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.2"},"vscode":{"interpreter":{"hash":"218548c374b4bbf341f954c1c86cc69d1fe99eef78085dfb9916d33ba2c70687"}}},"nbformat":4,"nbformat_minor":4}
